{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2c6e82",
   "metadata": {},
   "source": [
    "# Inference of the mit-b4 SegFormer\n",
    "\n",
    "This is a notebook used for performing the actual inference in the HuBMAP-HPA public and private test sets of the kaggle competition. It used different mask thresholds per organ category and its final Dice scores were:\n",
    "- Public Score: <span style=\"font-size: 130%;\"><b>0.57659</b></span>\n",
    "- Private Score: <span style=\"font-size: 130%;\"><b>0.50252</b></span>\n",
    "\n",
    "Some technical details:\n",
    "\n",
    "&minus; The competition required that there could be no internet connection, so the following dataset was used for installing the required libraries:\n",
    "[OpenMMLab Essential Repositories](https://www.kaggle.com/datasets/maxvandijck/openmmlab-essential-repositories)\n",
    "\n",
    "&minus; Additionally, the dataset with the produced mit-b4 SegFormer model was added: [Model SegFormer mitb4 93.22 mDice](https://www.kaggle.com/datasets/theo88/segformer-mitb4-93-mdice).\n",
    "\n",
    "&minus; In order to define different thresholds per organ the `MMSegmentation` original encoder-decoder code had to be tweeked. The alterations were taken from the [customize_mmseg](https://www.kaggle.com/code/opusen/customize-mmseg) notebook by [opusen](https://www.kaggle.com/opusen) and adapted to the `MMSegmentation` version used in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5df7be",
   "metadata": {},
   "source": [
    "## Installation of all the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adb31e6",
   "metadata": {
    "papermill": {
     "duration": 0.015383,
     "end_time": "2023-04-17T21:08:03.896391",
     "exception": false,
     "start_time": "2023-04-17T21:08:03.881008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-one\"></a>\n",
    "### 1. PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d4b201",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-17T21:08:03.935719Z",
     "iopub.status.busy": "2023-04-17T21:08:03.934915Z",
     "iopub.status.idle": "2023-04-17T21:09:41.933190Z",
     "shell.execute_reply": "2023-04-17T21:09:41.932152Z",
     "shell.execute_reply.started": "2023-04-17T21:03:27.317733Z"
    },
    "papermill": {
     "duration": 98.021594,
     "end_time": "2023-04-17T21:09:41.933377",
     "exception": false,
     "start_time": "2023-04-17T21:08:03.911783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/openmmlab-essential-repositories/openmmlab-repos/src/torch-1.10.0+cu111-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.10.0+cu111) (3.10.0.2)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.9.1\r\n",
      "    Uninstalling torch-1.9.1:\r\n",
      "      Successfully uninstalled torch-1.9.1\r\n",
      "Successfully installed torch-1.10.0+cu111\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# sys prerequisite import\n",
    "import sys\n",
    "# pytorch version compatible with openmmlab\n",
    "!pip install ../input/openmmlab-essential-repositories/openmmlab-repos/src/torch-1.10.0+cu111-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb506d0",
   "metadata": {
    "papermill": {
     "duration": 0.016933,
     "end_time": "2023-04-17T21:09:41.967756",
     "exception": false,
     "start_time": "2023-04-17T21:09:41.950823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-two\"></a>\n",
    "### 2. MMCV Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76fad1b1",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-17T21:09:42.012165Z",
     "iopub.status.busy": "2023-04-17T21:09:42.010877Z",
     "iopub.status.idle": "2023-04-17T21:11:38.157227Z",
     "shell.execute_reply": "2023-04-17T21:11:38.156735Z",
     "shell.execute_reply.started": "2023-04-17T21:03:54.675663Z"
    },
    "papermill": {
     "duration": 116.173411,
     "end_time": "2023-04-17T21:11:38.157371",
     "exception": false,
     "start_time": "2023-04-17T21:09:41.983960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/openmmlab-essential-repositories/openmmlab-repos/src/opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python==4.6.0.66) (1.19.5)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "  Attempting uninstall: opencv-python\r\n",
      "    Found existing installation: opencv-python 4.5.4.60\r\n",
      "    Uninstalling opencv-python-4.5.4.60:\r\n",
      "      Successfully uninstalled opencv-python-4.5.4.60\r\n",
      "Successfully installed opencv-python-4.6.0.66\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/openmmlab-essential-repositories/openmmlab-repos/src/yapf-0.32.0-py2.py3-none-any.whl\r\n",
      "Installing collected packages: yapf\r\n",
      "Successfully installed yapf-0.32.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/openmmlab-essential-repositories/openmmlab-repos/src/addict-2.4.0-py3-none-any.whl\r\n",
      "Installing collected packages: addict\r\n",
      "Successfully installed addict-2.4.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/openmmlab-essential-repositories/openmmlab-repos/src/mmcv_full-1.5.3-cp37-cp37m-manylinux1_x86_64.whl\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (21.0)\r\n",
      "Requirement already satisfied: addict in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (2.4.0)\r\n",
      "Requirement already satisfied: yapf in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (0.32.0)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (8.2.0)\r\n",
      "Requirement already satisfied: opencv-python>=3 in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (4.6.0.66)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (1.19.5)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mmcv-full==1.5.3) (6.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->mmcv-full==1.5.3) (3.0.6)\r\n",
      "Installing collected packages: mmcv-full\r\n",
      "Successfully installed mmcv-full-1.5.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# mmcv install\n",
    "!pip install ../input/openmmlab-essential-repositories/openmmlab-repos/src/opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "\n",
    "!pip install ../input/openmmlab-essential-repositories/openmmlab-repos/src/yapf-0.32.0-py2.py3-none-any.whl\n",
    "!pip install ../input/openmmlab-essential-repositories/openmmlab-repos/src/addict-2.4.0-py3-none-any.whl\n",
    "!pip install ../input/openmmlab-essential-repositories/openmmlab-repos/src/mmcv_full-1.5.3-cp37-cp37m-manylinux1_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e481e",
   "metadata": {
    "papermill": {
     "duration": 0.018787,
     "end_time": "2023-04-17T21:11:38.195768",
     "exception": false,
     "start_time": "2023-04-17T21:11:38.176981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-three\"></a>\n",
    "### 3. MMClassification Installation\n",
    "* **Requires MMCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ceb95bc",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-17T21:11:38.246746Z",
     "iopub.status.busy": "2023-04-17T21:11:38.244261Z",
     "iopub.status.idle": "2023-04-17T21:12:14.210378Z",
     "shell.execute_reply": "2023-04-17T21:12:14.210853Z",
     "shell.execute_reply.started": "2023-04-17T21:05:44.006342Z"
    },
    "papermill": {
     "duration": 35.996093,
     "end_time": "2023-04-17T21:12:14.211003",
     "exception": false,
     "start_time": "2023-04-17T21:11:38.214910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/mmclassification\n",
      "Obtaining file:///kaggle/working/mmclassification\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mmcls==0.23.1) (3.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmcls==0.23.1) (1.19.5)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmcls==0.23.1) (21.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (2.8.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (3.0.6)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (8.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (1.3.2)\r\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (6.3.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmcls==0.23.1) (4.28.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mmcls==0.23.1) (1.16.0)\r\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->mmcls==0.23.1) (1.2.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->mmcls==0.23.1) (59.1.1)\r\n",
      "Installing collected packages: mmcls\r\n",
      "  Running setup.py develop for mmcls\r\n",
      "Successfully installed mmcls-0.23.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# mmclassification install\n",
    "!cp -r ../input/openmmlab-essential-repositories/openmmlab-repos/mmclassification /kaggle/working\n",
    "%cd /kaggle/working/mmclassification\n",
    "!pip install -e .\n",
    "%cd ..\n",
    "\n",
    "sys.path.append('./mmclassification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d726b2",
   "metadata": {
    "papermill": {
     "duration": 0.0209,
     "end_time": "2023-04-17T21:12:14.302476",
     "exception": false,
     "start_time": "2023-04-17T21:12:14.281576",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"section-four\"></a>\n",
    "### 4. MMSegmentation Installation\n",
    "* **Requires MMCV**\n",
    "* **Requires MMClassification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa40089",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-17T21:12:14.347996Z",
     "iopub.status.busy": "2023-04-17T21:12:14.347202Z",
     "iopub.status.idle": "2023-04-17T21:12:51.876952Z",
     "shell.execute_reply": "2023-04-17T21:12:51.877401Z",
     "shell.execute_reply.started": "2023-04-17T21:06:24.872407Z"
    },
    "papermill": {
     "duration": 37.554057,
     "end_time": "2023-04-17T21:12:51.877593",
     "exception": false,
     "start_time": "2023-04-17T21:12:14.323536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/mmsegmentation\n",
      "Obtaining file:///kaggle/working/mmsegmentation\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mmsegmentation==0.26.0) (3.5.0)\r\n",
      "Requirement already satisfied: mmcls>=0.20.1 in /kaggle/working/mmclassification (from mmsegmentation==0.26.0) (0.23.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mmsegmentation==0.26.0) (1.19.5)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from mmsegmentation==0.26.0) (21.0)\r\n",
      "Requirement already satisfied: prettytable in /opt/conda/lib/python3.7/site-packages (from mmsegmentation==0.26.0) (2.4.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (1.3.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (2.8.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (3.0.6)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (0.11.0)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (8.2.0)\r\n",
      "Requirement already satisfied: setuptools-scm>=4 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (6.3.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mmsegmentation==0.26.0) (4.28.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from prettytable->mmsegmentation==0.26.0) (4.8.2)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prettytable->mmsegmentation==0.26.0) (0.2.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mmsegmentation==0.26.0) (1.16.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->mmsegmentation==0.26.0) (59.1.1)\r\n",
      "Requirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->mmsegmentation==0.26.0) (1.2.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->prettytable->mmsegmentation==0.26.0) (3.10.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->prettytable->mmsegmentation==0.26.0) (3.6.0)\r\n",
      "Installing collected packages: mmsegmentation\r\n",
      "  Running setup.py develop for mmsegmentation\r\n",
      "Successfully installed mmsegmentation-0.26.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "## mmsegmentation install\n",
    "!cp -r ../input/openmmlab-essential-repositories/openmmlab-repos/mmsegmentation /kaggle/working/\n",
    "%cd /kaggle/working/mmsegmentation\n",
    "!pip install -e .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95119ca1",
   "metadata": {},
   "source": [
    "## Tweaking the original encoder-decoder\n",
    "This teak was necessary in order for the model to return logits, so we could use them for different mask thresholds per organ category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d48a8b09",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-17T21:12:51.984731Z",
     "iopub.status.busy": "2023-04-17T21:12:51.983784Z",
     "iopub.status.idle": "2023-04-17T21:12:51.995149Z",
     "shell.execute_reply": "2023-04-17T21:12:51.994710Z",
     "shell.execute_reply.started": "2023-04-17T21:06:59.066010Z"
    },
    "papermill": {
     "duration": 0.037939,
     "end_time": "2023-04-17T21:12:51.995254",
     "exception": false,
     "start_time": "2023-04-17T21:12:51.957315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/mmsegmentation/mmseg/models/segmentors/encoder_decoder.py \n",
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mmseg.core import add_prefix\n",
    "from mmseg.ops import resize\n",
    "from .. import builder\n",
    "from ..builder import SEGMENTORS\n",
    "from .base import BaseSegmentor\n",
    "\n",
    "\n",
    "@SEGMENTORS.register_module()\n",
    "class EncoderDecoder(BaseSegmentor):\n",
    "    \"\"\"Encoder Decoder segmentors.\n",
    "\n",
    "    EncoderDecoder typically consists of backbone, decode_head, auxiliary_head.\n",
    "    Note that auxiliary_head is only used for deep supervision during training,\n",
    "    which could be dumped during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 backbone,\n",
    "                 decode_head,\n",
    "                 neck=None,\n",
    "                 auxiliary_head=None,\n",
    "                 train_cfg=None,\n",
    "                 test_cfg=None,\n",
    "                 pretrained=None,\n",
    "                 init_cfg=None):\n",
    "        super(EncoderDecoder, self).__init__(init_cfg)\n",
    "        if pretrained is not None:\n",
    "            assert backbone.get('pretrained') is None, \\\n",
    "                'both backbone and segmentor set pretrained weight'\n",
    "            backbone.pretrained = pretrained\n",
    "        self.backbone = builder.build_backbone(backbone)\n",
    "        if neck is not None:\n",
    "            self.neck = builder.build_neck(neck)\n",
    "        self._init_decode_head(decode_head)\n",
    "        self._init_auxiliary_head(auxiliary_head)\n",
    "\n",
    "        self.train_cfg = train_cfg\n",
    "        self.test_cfg = test_cfg\n",
    "\n",
    "        assert self.with_decode_head\n",
    "\n",
    "    def _init_decode_head(self, decode_head):\n",
    "        \"\"\"Initialize ``decode_head``\"\"\"\n",
    "        self.decode_head = builder.build_head(decode_head)\n",
    "        self.align_corners = self.decode_head.align_corners\n",
    "        self.num_classes = self.decode_head.num_classes\n",
    "\n",
    "    def _init_auxiliary_head(self, auxiliary_head):\n",
    "        \"\"\"Initialize ``auxiliary_head``\"\"\"\n",
    "        if auxiliary_head is not None:\n",
    "            if isinstance(auxiliary_head, list):\n",
    "                self.auxiliary_head = nn.ModuleList()\n",
    "                for head_cfg in auxiliary_head:\n",
    "                    self.auxiliary_head.append(builder.build_head(head_cfg))\n",
    "            else:\n",
    "                self.auxiliary_head = builder.build_head(auxiliary_head)\n",
    "\n",
    "    def extract_feat(self, img):\n",
    "        \"\"\"Extract features from images.\"\"\"\n",
    "        x = self.backbone(img)\n",
    "        if self.with_neck:\n",
    "            x = self.neck(x)\n",
    "        return x\n",
    "\n",
    "    def encode_decode(self, img, img_metas):\n",
    "        \"\"\"Encode images with backbone and decode into a semantic segmentation\n",
    "        map of the same size as input.\"\"\"\n",
    "        x = self.extract_feat(img)\n",
    "        out = self._decode_head_forward_test(x, img_metas)\n",
    "        out = resize(\n",
    "            input=out,\n",
    "            size=img.shape[2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=self.align_corners)\n",
    "        return out\n",
    "\n",
    "    def _decode_head_forward_train(self, x, img_metas, gt_semantic_seg):\n",
    "        \"\"\"Run forward function and calculate loss for decode head in\n",
    "        training.\"\"\"\n",
    "        losses = dict()\n",
    "        loss_decode = self.decode_head.forward_train(x, img_metas,\n",
    "                                                     gt_semantic_seg,\n",
    "                                                     self.train_cfg)\n",
    "\n",
    "        losses.update(add_prefix(loss_decode, 'decode'))\n",
    "        return losses\n",
    "\n",
    "    def _decode_head_forward_test(self, x, img_metas):\n",
    "        \"\"\"Run forward function and calculate loss for decode head in\n",
    "        inference.\"\"\"\n",
    "        seg_logits = self.decode_head.forward_test(x, img_metas, self.test_cfg)\n",
    "        return seg_logits\n",
    "\n",
    "    def _auxiliary_head_forward_train(self, x, img_metas, gt_semantic_seg):\n",
    "        \"\"\"Run forward function and calculate loss for auxiliary head in\n",
    "        training.\"\"\"\n",
    "        losses = dict()\n",
    "        if isinstance(self.auxiliary_head, nn.ModuleList):\n",
    "            for idx, aux_head in enumerate(self.auxiliary_head):\n",
    "                loss_aux = aux_head.forward_train(x, img_metas,\n",
    "                                                  gt_semantic_seg,\n",
    "                                                  self.train_cfg)\n",
    "                losses.update(add_prefix(loss_aux, f'aux_{idx}'))\n",
    "        else:\n",
    "            loss_aux = self.auxiliary_head.forward_train(\n",
    "                x, img_metas, gt_semantic_seg, self.train_cfg)\n",
    "            losses.update(add_prefix(loss_aux, 'aux'))\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def forward_dummy(self, img):\n",
    "        \"\"\"Dummy forward function.\"\"\"\n",
    "        seg_logit = self.encode_decode(img, None)\n",
    "\n",
    "        return seg_logit\n",
    "\n",
    "    def forward_train(self, img, img_metas, gt_semantic_seg):\n",
    "        \"\"\"Forward function for training.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): Input images.\n",
    "            img_metas (list[dict]): List of image info dict where each dict\n",
    "                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n",
    "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n",
    "                For details on the values of these keys see\n",
    "                `mmseg/datasets/pipelines/formatting.py:Collect`.\n",
    "            gt_semantic_seg (Tensor): Semantic segmentation masks\n",
    "                used if the architecture supports semantic segmentation task.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Tensor]: a dictionary of loss components\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.extract_feat(img)\n",
    "\n",
    "        losses = dict()\n",
    "\n",
    "        loss_decode = self._decode_head_forward_train(x, img_metas,\n",
    "                                                      gt_semantic_seg)\n",
    "        losses.update(loss_decode)\n",
    "\n",
    "        if self.with_auxiliary_head:\n",
    "            loss_aux = self._auxiliary_head_forward_train(\n",
    "                x, img_metas, gt_semantic_seg)\n",
    "            losses.update(loss_aux)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # TODO refactor\n",
    "    def slide_inference(self, img, img_meta, rescale):\n",
    "        \"\"\"Inference by sliding-window with overlap.\n",
    "\n",
    "        If h_crop > h_img or w_crop > w_img, the small patch will be used to\n",
    "        decode without padding.\n",
    "        \"\"\"\n",
    "\n",
    "        h_stride, w_stride = self.test_cfg.stride\n",
    "        h_crop, w_crop = self.test_cfg.crop_size\n",
    "        batch_size, _, h_img, w_img = img.size()\n",
    "        num_classes = self.num_classes\n",
    "        h_grids = max(h_img - h_crop + h_stride - 1, 0) // h_stride + 1\n",
    "        w_grids = max(w_img - w_crop + w_stride - 1, 0) // w_stride + 1\n",
    "        preds = img.new_zeros((batch_size, num_classes, h_img, w_img))\n",
    "        count_mat = img.new_zeros((batch_size, 1, h_img, w_img))\n",
    "        for h_idx in range(h_grids):\n",
    "            for w_idx in range(w_grids):\n",
    "                y1 = h_idx * h_stride\n",
    "                x1 = w_idx * w_stride\n",
    "                y2 = min(y1 + h_crop, h_img)\n",
    "                x2 = min(x1 + w_crop, w_img)\n",
    "                y1 = max(y2 - h_crop, 0)\n",
    "                x1 = max(x2 - w_crop, 0)\n",
    "                crop_img = img[:, :, y1:y2, x1:x2]\n",
    "                crop_seg_logit = self.encode_decode(crop_img, img_meta)\n",
    "                preds += F.pad(crop_seg_logit,\n",
    "                               (int(x1), int(preds.shape[3] - x2), int(y1),\n",
    "                                int(preds.shape[2] - y2)))\n",
    "\n",
    "                count_mat[:, :, y1:y2, x1:x2] += 1\n",
    "        assert (count_mat == 0).sum() == 0\n",
    "        if torch.onnx.is_in_onnx_export():\n",
    "            # cast count_mat to constant while exporting to ONNX\n",
    "            count_mat = torch.from_numpy(\n",
    "                count_mat.cpu().detach().numpy()).to(device=img.device)\n",
    "        preds = preds / count_mat\n",
    "        if rescale:\n",
    "            # remove padding area\n",
    "#             resize_shape = img_meta[0]['img_shape'][:2]\n",
    "#             preds = preds[:, :, :resize_shape[0], :resize_shape[1]]\n",
    "            preds = resize(\n",
    "                preds,\n",
    "                size=img_meta[0]['ori_shape'][:2],\n",
    "                mode='bilinear',\n",
    "                align_corners=self.align_corners,\n",
    "                warning=False)\n",
    "        return preds\n",
    "\n",
    "    def whole_inference(self, img, img_meta, rescale):\n",
    "        \"\"\"Inference with full image.\"\"\"\n",
    "\n",
    "        seg_logit = self.encode_decode(img, img_meta)\n",
    "        if rescale:\n",
    "            # support dynamic shape for onnx\n",
    "            if torch.onnx.is_in_onnx_export():\n",
    "                size = img.shape[2:]\n",
    "            else:\n",
    "                # remove padding area\n",
    "#                 resize_shape = img_meta[0]['img_shape'][:2]\n",
    "#                 seg_logit = seg_logit[:, :, :resize_shape[0], :resize_shape[1]]\n",
    "                size = img_meta[0]['ori_shape'][:2]\n",
    "            seg_logit = resize(\n",
    "                seg_logit,\n",
    "                size=size,\n",
    "                mode='bilinear',\n",
    "                align_corners=self.align_corners,\n",
    "                warning=False)\n",
    "\n",
    "        return seg_logit\n",
    "\n",
    "    def inference(self, img, img_meta, rescale):\n",
    "        \"\"\"Inference with slide/whole style.\n",
    "\n",
    "        Args:\n",
    "            img (Tensor): The input image of shape (N, 3, H, W).\n",
    "            img_meta (dict): Image info dict where each dict has: 'img_shape',\n",
    "                'scale_factor', 'flip', and may also contain\n",
    "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n",
    "                For details on the values of these keys see\n",
    "                `mmseg/datasets/pipelines/formatting.py:Collect`.\n",
    "            rescale (bool): Whether rescale back to original shape.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output segmentation map.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.test_cfg.mode in ['slide', 'whole']\n",
    "        ori_shape = img_meta[0]['ori_shape']\n",
    "        assert all(_['ori_shape'] == ori_shape for _ in img_meta)\n",
    "        if self.test_cfg.mode == 'slide':\n",
    "            seg_logit = self.slide_inference(img, img_meta, rescale)\n",
    "        else:\n",
    "            seg_logit = self.whole_inference(img, img_meta, rescale)\n",
    "\n",
    "        if self.test_cfg.get('sigmoid', False):\n",
    "            output = F.sigmoid(seg_logit)\n",
    "        else:\n",
    "            output = F.softmax(seg_logit, dim=1)\n",
    "        flip = img_meta[0]['flip']\n",
    "        if flip:\n",
    "            flip_direction = img_meta[0]['flip_direction']\n",
    "            assert flip_direction in ['horizontal', 'vertical']\n",
    "            if flip_direction == 'horizontal':\n",
    "                output = output.flip(dims=(3, ))\n",
    "            elif flip_direction == 'vertical':\n",
    "                output = output.flip(dims=(2, ))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def simple_test(self, img, img_meta, rescale=True):\n",
    "        \"\"\"Simple test with single image.\"\"\"\n",
    "        seg_logit = self.inference(img, img_meta, rescale)\n",
    "        if self.test_cfg.get('sigmoid', False):\n",
    "            seg_pred = seg_logit\n",
    "        else:\n",
    "            seg_pred = seg_logit.argmax(dim=1)\n",
    "        if torch.onnx.is_in_onnx_export():\n",
    "            # our inference backend only support 4D output\n",
    "            seg_pred = seg_pred.unsqueeze(0)\n",
    "            return seg_pred\n",
    "        seg_pred = seg_pred.cpu().numpy()\n",
    "        # unravel batch dim\n",
    "        seg_pred = list(seg_pred)\n",
    "        return seg_pred\n",
    "\n",
    "    def aug_test(self, imgs, img_metas, rescale=True):\n",
    "        \"\"\"Test with augmentations.\n",
    "\n",
    "        Only rescale=True is supported.\n",
    "        \"\"\"\n",
    "        # aug_test rescale all imgs back to ori_shape for now\n",
    "        assert rescale\n",
    "        # to save memory, we get augmented seg logit inplace\n",
    "        seg_logit = self.inference(imgs[0], img_metas[0], rescale)\n",
    "        for i in range(1, len(imgs)):\n",
    "            cur_seg_logit = self.inference(imgs[i], img_metas[i], rescale)\n",
    "            seg_logit += cur_seg_logit\n",
    "        seg_logit /= len(imgs)\n",
    "        if self.test_cfg.get('sigmoid', False):\n",
    "            seg_pred = seg_logit\n",
    "        else:\n",
    "            seg_pred = seg_logit.argmax(dim=1)\n",
    "        seg_pred = seg_pred.cpu().numpy()\n",
    "        # unravel batch dim\n",
    "        seg_pred = list(seg_pred)\n",
    "        return seg_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19fd663",
   "metadata": {
    "papermill": {
     "duration": 0.024275,
     "end_time": "2023-04-17T21:12:52.043443",
     "exception": false,
     "start_time": "2023-04-17T21:12:52.019168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Performing Inference with different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3101349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:12:52.096378Z",
     "iopub.status.busy": "2023-04-17T21:12:52.094197Z",
     "iopub.status.idle": "2023-04-17T21:12:56.438240Z",
     "shell.execute_reply": "2023-04-17T21:12:56.439050Z",
     "shell.execute_reply.started": "2023-04-17T21:06:59.085348Z"
    },
    "papermill": {
     "duration": 4.371761,
     "end_time": "2023-04-17T21:12:56.439233",
     "exception": false,
     "start_time": "2023-04-17T21:12:52.067472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "sys.path.append('./mmsegmentation')\n",
    "\n",
    "from mmseg.apis import init_segmentor, inference_segmentor\n",
    "from mmcv.utils import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef973402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:12:56.494804Z",
     "iopub.status.busy": "2023-04-17T21:12:56.493172Z",
     "iopub.status.idle": "2023-04-17T21:12:56.495447Z",
     "shell.execute_reply": "2023-04-17T21:12:56.495874Z",
     "shell.execute_reply.started": "2023-04-17T21:06:59.095050Z"
    },
    "papermill": {
     "duration": 0.031972,
     "end_time": "2023-04-17T21:12:56.496015",
     "exception": false,
     "start_time": "2023-04-17T21:12:56.464043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rle_encode_less_memory(img):\n",
    "    pixels = img.T.flatten()\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8522b",
   "metadata": {},
   "source": [
    "We have to change the original configuration file in order for the tweak to be used by setting `test_cfg=dict(mode='whole', sigmoid=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ae6ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:12:56.552775Z",
     "iopub.status.busy": "2023-04-17T21:12:56.551883Z",
     "iopub.status.idle": "2023-04-17T21:13:01.237389Z",
     "shell.execute_reply": "2023-04-17T21:13:01.236911Z",
     "shell.execute_reply.started": "2023-04-17T21:06:59.107189Z"
    },
    "papermill": {
     "duration": 4.71731,
     "end_time": "2023-04-17T21:13:01.237554",
     "exception": false,
     "start_time": "2023-04-17T21:12:56.520244",
     "status": "completed"
    }
   },
   "outputs": [],
   "source": [
    "config_file = '/kaggle/input/segformer-mitb4-93-mdice/segformer_HuBMAP.py'\n",
    "new_config = '/kaggle/working/segformer_HuBMAP.py'\n",
    "!cp {config_file} {new_config}\n",
    "!sed -i \"s/test_cfg=dict(mode='whole'))/test_cfg=dict(mode='whole', sigmoid=True))/\" {new_config}\n",
    "# Correcting a typo in the configuration file of the model dataset\n",
    "!sed -i \"s/num_classes=6/num_classes=2/\" {new_config}\n",
    "!grep test_cfg {new_config}\n",
    "!grep num_classes {new_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac9d766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:01.293125Z",
     "iopub.status.busy": "2023-04-17T21:13:01.292589Z",
     "iopub.status.idle": "2023-04-17T21:13:08.470006Z",
     "shell.execute_reply": "2023-04-17T21:13:08.469479Z",
     "shell.execute_reply.started": "2023-04-17T21:07:04.065320Z"
    },
    "papermill": {
     "duration": 7.207669,
     "end_time": "2023-04-17T21:13:08.470151",
     "exception": false,
     "start_time": "2023-04-17T21:13:01.262482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:236: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  'Default ``avg_non_ignore`` is False, if you would like to '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /kaggle/input/segformer-mitb4-93-mdice/SegFormer_mitb4_1024x1024_9322_No_Albu.pth\n"
     ]
    }
   ],
   "source": [
    "DATA = '/kaggle/input/hubmap-organ-segmentation/test_images/'\n",
    "df_sample = pd.read_csv('/kaggle/input/hubmap-organ-segmentation/sample_submission.csv').set_index('id')\n",
    "df_test = pd.read_csv('/kaggle/input/hubmap-organ-segmentation/test.csv')\n",
    "checkpoint_file = '/kaggle/input/segformer-mitb4-93-mdice/SegFormer_mitb4_1024x1024_9322_No_Albu.pth'\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "model = init_segmentor(new_config, checkpoint_file, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80412cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:08.525617Z",
     "iopub.status.busy": "2023-04-17T21:13:08.524970Z",
     "iopub.status.idle": "2023-04-17T21:13:08.527710Z",
     "shell.execute_reply": "2023-04-17T21:13:08.527252Z",
     "shell.execute_reply.started": "2023-04-17T21:07:06.583274Z"
    },
    "papermill": {
     "duration": 0.031797,
     "end_time": "2023-04-17T21:13:08.527830",
     "exception": false,
     "start_time": "2023-04-17T21:13:08.496033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds={\n",
    "    \"kidney\":0.225,\n",
    "    \"prostate\":0.225,\n",
    "    \"largeintestine\":0.225,\n",
    "    \"spleen\":0.1,\n",
    "    \"lung\":0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad72f37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:08.585943Z",
     "iopub.status.busy": "2023-04-17T21:13:08.584618Z",
     "iopub.status.idle": "2023-04-17T21:13:11.385256Z",
     "shell.execute_reply": "2023-04-17T21:13:11.385690Z",
     "shell.execute_reply.started": "2023-04-17T21:07:06.593022Z"
    },
    "papermill": {
     "duration": 2.832812,
     "end_time": "2023-04-17T21:13:11.385859",
     "exception": false,
     "start_time": "2023-04-17T21:13:08.553047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4321443fd3b2414f8125eb0de3e69686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1806: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "names,preds = [],[]\n",
    "imgs, pd_mks = [],[]\n",
    "debug = len(df_sample)<2\n",
    "for idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n",
    "    img_file = os.path.join(DATA,str(idx)+'.tiff')\n",
    "    img_inf = inference_segmentor(model, img_file)\n",
    "    pred = img_inf[0]\n",
    "    organ = df_test.loc[df_test.id==idx, 'organ'].item()\n",
    "    thresh = thresholds[organ]\n",
    "    pred = (pred[1]>=thresh).astype(np.uint8)\n",
    "    rle = rle_encode_less_memory(pred)\n",
    "    names.append(str(idx))\n",
    "    preds.append(rle)\n",
    "    if debug:\n",
    "        img  = cv2.imread(os.path.join(DATA,str(idx)+'.tiff'))\n",
    "        imgs.append(img)\n",
    "        pd_mks.append(pred)\n",
    "    del img_inf, pred, rle, idx, row\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924411e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:11.445255Z",
     "iopub.status.busy": "2023-04-17T21:13:11.444659Z",
     "iopub.status.idle": "2023-04-17T21:13:11.447105Z",
     "shell.execute_reply": "2023-04-17T21:13:11.447551Z",
     "shell.execute_reply.started": "2023-04-17T21:07:10.050295Z"
    },
    "papermill": {
     "duration": 0.034946,
     "end_time": "2023-04-17T21:13:11.447716",
     "exception": false,
     "start_time": "2023-04-17T21:13:11.412770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mmseg.apis import show_result_pyplot\n",
    "    for img, mask in zip(imgs, pd_mks):\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.subplot(1, 3, 1); plt.imshow(img); plt.axis('OFF'); plt.title('image')\n",
    "        plt.subplot(1, 3, 2); plt.imshow(mask*255); plt.axis('OFF'); plt.title('mask')\n",
    "        plt.subplot(1, 3, 3); plt.imshow(img); plt.imshow(mask*255, alpha=0.4); plt.axis('OFF'); plt.title('overlay')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb4d58f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:11.504823Z",
     "iopub.status.busy": "2023-04-17T21:13:11.503881Z",
     "iopub.status.idle": "2023-04-17T21:13:12.576601Z",
     "shell.execute_reply": "2023-04-17T21:13:12.577377Z",
     "shell.execute_reply.started": "2023-04-17T21:07:10.060018Z"
    },
    "papermill": {
     "duration": 1.103446,
     "end_time": "2023-04-17T21:13:12.577568",
     "exception": false,
     "start_time": "2023-04-17T21:13:11.474122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "668eb572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T21:13:12.636523Z",
     "iopub.status.busy": "2023-04-17T21:13:12.635781Z",
     "iopub.status.idle": "2023-04-17T21:13:12.643356Z",
     "shell.execute_reply": "2023-04-17T21:13:12.643766Z",
     "shell.execute_reply.started": "2023-04-17T21:07:11.128871Z"
    },
    "papermill": {
     "duration": 0.040095,
     "end_time": "2023-04-17T21:13:12.643906",
     "exception": false,
     "start_time": "2023-04-17T21:13:12.603811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'rle':preds})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 318.062686,
   "end_time": "2023-04-17T21:13:14.298366",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-17T21:07:56.235680",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00518dcf87cb4799b695ccd7f2edd7ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2f2218596d6c4459a57469c710873a64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53600009d6b042549b63b1bcc265965e",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_00518dcf87cb4799b695ccd7f2edd7ee",
       "value": 1
      }
     },
     "3fbced617e304f048537187bd1b035ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4321443fd3b2414f8125eb0de3e69686": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f0ce4d1ee01e455091becdb18d62ac7f",
        "IPY_MODEL_2f2218596d6c4459a57469c710873a64",
        "IPY_MODEL_fc02e492490e4c6ca430b221875413db"
       ],
       "layout": "IPY_MODEL_eb7e374d9c824b6eb9e26871d8bbcb46"
      }
     },
     "53600009d6b042549b63b1bcc265965e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "72e7e7df6ec64ea2a5541262b66fdc21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9e21ed6f983044498b6ed3538b61ac6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cc2781727f974b0ab65877939082e754": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb7e374d9c824b6eb9e26871d8bbcb46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f0ce4d1ee01e455091becdb18d62ac7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_72e7e7df6ec64ea2a5541262b66fdc21",
       "placeholder": "​",
       "style": "IPY_MODEL_9e21ed6f983044498b6ed3538b61ac6e",
       "value": "100%"
      }
     },
     "fc02e492490e4c6ca430b221875413db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cc2781727f974b0ab65877939082e754",
       "placeholder": "​",
       "style": "IPY_MODEL_3fbced617e304f048537187bd1b035ae",
       "value": " 1/1 [00:02&lt;00:00,  2.79s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
